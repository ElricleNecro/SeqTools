{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and running a preprocessing pipeline\n",
    "\n",
    "In this example, an image processing pipeline is created and then executed in a manner that maximize throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from PIL import Image, ImageOps\n",
    "import seqtools\n",
    "\n",
    "workdir = tempfile.TemporaryDirectory()\n",
    "os.chdir(workdir.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -s \"https://cdn.pixabay.com/photo/2017/04/07/01/05/owl-2209827_640.jpg\" -o owl.jpg\n",
    "! curl -s \"https://cdn.pixabay.com/photo/2018/08/26/14/05/hahn-3632299_640.jpg\" -o rooster.jpg\n",
    "! curl -s \"https://cdn.pixabay.com/photo/2018/09/02/10/03/violet-duck-3648415_640.jpg\" -o duck.jpg\n",
    "! curl -s \"https://cdn.pixabay.com/photo/2018/08/21/05/15/tit-3620632_640.jpg\" -o bird.jpg\n",
    "! curl -s \"https://cdn.pixabay.com/photo/2018/09/04/18/07/pug-3654360_640.jpg\" -o dog.jpg\n",
    "! curl -s \"https://cdn.pixabay.com/photo/2018/09/04/18/52/hedgehog-3654434_640.jpg\" -o hedgehog.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial data loading\n",
    "\n",
    "SeqTools works with list-like indexable objects, so the first step is to create one that maps to our samples, then this object will be passed to functions that apply the desired transformations.\n",
    "In this example, we represent our samples with their file names and store them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['owl', 'rooster', 'duck', 'bird', 'dog', 'hedgehog']\n",
    "# We artificially increase the size of the dataset for the example\n",
    "labels = [labels[i % len(labels)] for i in range(1000)]\n",
    "\n",
    "image_files = [l + '.jpg' for l in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the full resolution images, the result cannot normally fit into memory, but with SeqTools the evaluation is delayed until the images are actually accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_images = seqtools.smap(Image.open, image_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify the result for one sample, this will trigger its evaluation and return it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Mapping transformations\n",
    "\n",
    "As a first preprocessing stage, we can normalize the size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_size(im):\n",
    "    w, h = im.size\n",
    "    left_crop = w // 2 - h // 2\n",
    "    return im.resize((200, 200), Image.BILINEAR, box=(left_crop, 1, h, h))\n",
    "\n",
    "small_images = seqtools.smap(normalize_size, raw_images)\n",
    "\n",
    "small_images[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then apply common preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrasted = seqtools.smap(ImageOps.autocontrast, small_images)\n",
    "equalized = seqtools.smap(ImageOps.equalize, contrasted)\n",
    "grayscale = seqtools.smap(ImageOps.grayscale, equalized)\n",
    "\n",
    "grayscale[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That preprocessing seems a bit over the top... let's check where it went wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrasted[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sample, the minimal set of computations was run to produce the requested item.\n",
    "We find here that equalization is inappropriate and autocontrast is too weak, let's fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale = seqtools.smap(ImageOps.grayscale, small_images)\n",
    "contrasted = seqtools.smap(lambda im: ImageOps.autocontrast(im, cutoff=3), grayscale)\n",
    "\n",
    "contrasted[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining datasets\n",
    "\n",
    "Then we want to augment the dataset by flipping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate flipped versions of the images\n",
    "flipped = seqtools.smap(ImageOps.mirror, contrasted)\n",
    "\n",
    "# Combine with the original dataset\n",
    "augmented_dataset = seqtools.concatenate([contrasted, flipped])\n",
    "\n",
    "augmented_dataset[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Once satisfied with our preprocessing pipeline, evaluating all values is simply done by iterating over the elements or forcing the conversion to a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time computed_values = list(augmented_dataset);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above evaluation is a bit slow, probably due to the IO operations when loading the images from the hard drive. Maybe using multiple threads could help keep the CPU busy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_dataset = seqtools.prefetch(augmented_dataset, max_buffered=10, nworkers=2)\n",
    "%time computed_values = list(fast_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CPU time is the same because the computations are the same (plus some threading overhead), but the wall time was cut by half because image processing continues for some images while others are being loaded.\n",
    "\n",
    "However, we could spare some IO by not reading the same image a second time when generating the augmented version, and also save some common transformations. For that matter, we can put a cache to save intermediate values within the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_images = seqtools.smap(Image.open, image_files)\n",
    "small_images = seqtools.smap(normalize_size, raw_images)\n",
    "grayscale = seqtools.smap(ImageOps.grayscale, small_images)\n",
    "contrasted = seqtools.smap(lambda im: ImageOps.autocontrast(im, cutoff=3), grayscale)\n",
    "\n",
    "# Use some cache to avoid recomputation of recently accessed item.\n",
    "contrasted = seqtools.add_cache(contrasted, cache_size=20)\n",
    "\n",
    "flipped = seqtools.smap(ImageOps.mirror, contrasted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need organize the calls to items of `grayscale` so that the original image is found in the cache when generating the flipped one.\n",
    "Besides, we need to to make sure that we don't have simultaneous calls between workers, otherwise they will have to wait for the first one to finish.\n",
    "\n",
    "To do so, we simply compute related images by groups and then flatten the results into a long sequence of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = seqtools.collate([contrasted, flipped])\n",
    "flattened = seqtools.unbatch(grouped, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_dataset = seqtools.prefetch(flattened, max_buffered=10, nworkers=2)\n",
    "%time computed_values = list(fast_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wall time did not improve much but the CPU time was cut in half, leaving more room for other processes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
